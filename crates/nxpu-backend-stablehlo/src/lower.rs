//! StableHLO MLIR text emission from classified kernel patterns.
//!
//! Emits valid MLIR textual format with StableHLO dialect operations
//! targeting Google Cloud TPU via the OpenXLA compiler stack.

use nxpu_backend_onnx::analyze::{
    ActivationOp, ElementWiseOp, KernelPattern, PoolKind, ReduceOp, TensorBinding,
};
use nxpu_backend_onnx::proto::data_type;

/// Build StableHLO MLIR text from a classified kernel pattern.
pub fn build_mlir(pattern: &KernelPattern, ep_name: &str) -> String {
    match pattern {
        KernelPattern::MatMul {
            inputs,
            output,
            shape,
        } => build_matmul_mlir(&inputs[0], &inputs[1], output, shape, ep_name),
        KernelPattern::ElementWise {
            op,
            inputs,
            output,
            dim_name,
        } => build_elementwise_mlir(*op, &inputs[0], &inputs[1], output, dim_name, ep_name),
        KernelPattern::Conv2D {
            input,
            weight,
            output,
            ..
        } => build_conv2d_mlir(input, weight, output, ep_name),
        KernelPattern::Pool {
            kind,
            input,
            output,
            ..
        } => build_pool_mlir(*kind, input, output, ep_name),
        KernelPattern::Activation {
            op,
            input,
            output,
            dim_name,
        } => build_activation_mlir(*op, input, output, dim_name, ep_name),
        KernelPattern::Reduce {
            op,
            input,
            output,
            axis,
        } => build_reduce_mlir(*op, input, output, *axis, ep_name),
        KernelPattern::Transpose {
            input,
            output,
            perm,
        } => build_transpose_mlir(input, output, perm, ep_name),
        KernelPattern::Reshape { input, output, .. } => build_reshape_mlir(input, output, ep_name),
        KernelPattern::Normalization {
            input,
            scale,
            bias,
            output,
            ..
        } => build_normalization_mlir(input, scale, bias, output, ep_name),
    }
}

fn onnx_to_mlir_type(onnx_dt: i32) -> &'static str {
    match onnx_dt {
        data_type::FLOAT => "f32",
        data_type::FLOAT16 => "f16",
        data_type::BFLOAT16 => "bf16",
        data_type::INT32 => "i32",
        data_type::INT8 => "i8",
        data_type::UINT32 => "ui32",
        data_type::BOOL => "i1",
        _ => "f32",
    }
}

fn build_matmul_mlir(
    a: &TensorBinding,
    b: &TensorBinding,
    c: &TensorBinding,
    shape: &nxpu_backend_onnx::analyze::MatMulShape,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(a.elem_type);
    let a_type = format!("tensor<?x?x{ty}>");
    let b_type = format!("tensor<?x?x{ty}>");
    let c_type = format!("tensor<?x?x{ty}>");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: MatMul [{m},{k}] x [{k},{n}] -> [{m},{n}]
module @{ep_name} {{
  func.func @main(%{a}: {a_type}, %{b}: {b_type}) -> {c_type} {{
    %{c} = stablehlo.dot_general %{a}, %{b},
      batching_dims = [] x [],
      contracting_dims = [1] x [0]
      : ({a_type}, {b_type}) -> {c_type}
    return %{c} : {c_type}
  }}
}}
"#,
        ep_name = ep_name,
        m = shape.m,
        n = shape.n,
        k = shape.k,
        a = a.name,
        b = b.name,
        c = c.name,
        a_type = a_type,
        b_type = b_type,
        c_type = c_type,
    )
}

fn build_elementwise_mlir(
    op: ElementWiseOp,
    a: &TensorBinding,
    b: &TensorBinding,
    c: &TensorBinding,
    dim_name: &str,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(a.elem_type);
    let tensor_type = format!("tensor<?x{ty}>");

    let stablehlo_op = match op {
        ElementWiseOp::Add => "stablehlo.add",
        ElementWiseOp::Sub => "stablehlo.subtract",
        ElementWiseOp::Mul => "stablehlo.multiply",
        ElementWiseOp::Div => "stablehlo.divide",
    };

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: {op_name} [{dim}]
module @{ep_name} {{
  func.func @main(%{a}: {t}, %{b}: {t}) -> {t} {{
    %{c} = {stablehlo_op} %{a}, %{b} : {t}
    return %{c} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        op_name = op.onnx_op_type(),
        dim = dim_name,
        a = a.name,
        b = b.name,
        c = c.name,
        t = tensor_type,
        stablehlo_op = stablehlo_op,
    )
}

fn build_conv2d_mlir(
    input: &TensorBinding,
    weight: &TensorBinding,
    output: &TensorBinding,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let in_type = format!("tensor<?x?x?x?x{ty}>");
    let w_type = format!("tensor<?x?x?x?x{ty}>");
    let out_type = format!("tensor<?x?x?x?x{ty}>");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Conv2D
module @{ep_name} {{
  func.func @main(%{inp}: {in_type}, %{w}: {w_type}) -> {out_type} {{
    %{out} = stablehlo.convolution(%{inp}, %{w})
      dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1],
      window = {{stride = [1, 1], pad = [[0, 0], [0, 0]]}}
      : ({in_type}, {w_type}) -> {out_type}
    return %{out} : {out_type}
  }}
}}
"#,
        ep_name = ep_name,
        inp = input.name,
        w = weight.name,
        out = output.name,
    )
}

fn build_pool_mlir(
    kind: PoolKind,
    input: &TensorBinding,
    output: &TensorBinding,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let tensor_type = format!("tensor<?x?x?x?x{ty}>");
    let pool_name = match kind {
        PoolKind::Max => "max",
        PoolKind::Avg => "avg",
    };

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: {pool_name}_pool
module @{ep_name} {{
  func.func @main(%{inp}: {t}) -> {t} {{
    %{out} = stablehlo.reduce_window(%{inp})
      window_dimensions = [1, 1, 2, 2],
      window_strides = [1, 1, 2, 2]
      : ({t}) -> {t}
    return %{out} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        pool_name = pool_name,
        inp = input.name,
        out = output.name,
        t = tensor_type,
    )
}

fn build_activation_mlir(
    op: ActivationOp,
    input: &TensorBinding,
    output: &TensorBinding,
    dim_name: &str,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let tensor_type = format!("tensor<?x{ty}>");

    let (op_line, op_name) = match op {
        ActivationOp::Relu => (
            format!(
                "    %zero = stablehlo.constant dense<0.0> : {t}\n    %{out} = stablehlo.maximum %{inp}, %zero : {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Relu",
        ),
        ActivationOp::Sigmoid => (
            format!(
                "    %{out} = stablehlo.logistic %{inp} : {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Sigmoid",
        ),
        ActivationOp::Tanh => (
            format!(
                "    %{out} = stablehlo.tanh %{inp} : {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Tanh",
        ),
        ActivationOp::Softmax => (
            format!(
                "    %{out} = stablehlo.custom_call @softmax(%{inp}) : ({t}) -> {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Softmax",
        ),
    };

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: {op_name} [{dim}]
module @{ep_name} {{
  func.func @main(%{inp}: {t}) -> {t} {{
{op_line}
    return %{out} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        op_name = op_name,
        dim = dim_name,
        inp = input.name,
        out = output.name,
        t = tensor_type,
        op_line = op_line,
    )
}

fn build_reduce_mlir(
    op: ReduceOp,
    input: &TensorBinding,
    output: &TensorBinding,
    axis: i64,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let in_type = format!("tensor<?x?x{ty}>");
    let out_type = format!("tensor<?x{ty}>");

    let reduce_body = match op {
        ReduceOp::Sum | ReduceOp::Mean => "stablehlo.add",
        ReduceOp::Max => "stablehlo.maximum",
        ReduceOp::Min => "stablehlo.minimum",
    };

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: {op_name} axis={axis}
module @{ep_name} {{
  func.func @main(%{inp}: {in_type}) -> {out_type} {{
    %init = stablehlo.constant dense<0.0> : tensor<{ty}>
    %{out} = stablehlo.reduce(%{inp} init: %init) applies {reduce_body} across dimensions = [{axis}] : ({in_type}, tensor<{ty}>) -> {out_type}
    return %{out} : {out_type}
  }}
}}
"#,
        ep_name = ep_name,
        op_name = op.onnx_op_type(),
        inp = input.name,
        out = output.name,
    )
}

fn build_transpose_mlir(
    input: &TensorBinding,
    output: &TensorBinding,
    perm: &[i64],
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let ndim = perm.len();
    let dims_str = (0..ndim).map(|_| "?").collect::<Vec<_>>().join("x");
    let tensor_type = format!("tensor<{dims_str}x{ty}>");
    let perm_str = perm
        .iter()
        .map(|p| p.to_string())
        .collect::<Vec<_>>()
        .join(", ");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Transpose perm=[{perm_str}]
module @{ep_name} {{
  func.func @main(%{inp}: {t}) -> {t} {{
    %{out} = stablehlo.transpose %{inp}, dims = [{perm_str}] : ({t}) -> {t}
    return %{out} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        inp = input.name,
        out = output.name,
        t = tensor_type,
    )
}

fn build_reshape_mlir(input: &TensorBinding, output: &TensorBinding, ep_name: &str) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let tensor_type = format!("tensor<?x{ty}>");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Reshape
module @{ep_name} {{
  func.func @main(%{inp}: {t}) -> {t} {{
    %{out} = stablehlo.reshape %{inp} : ({t}) -> {t}
    return %{out} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        inp = input.name,
        out = output.name,
        t = tensor_type,
    )
}

fn build_normalization_mlir(
    input: &TensorBinding,
    scale: &TensorBinding,
    bias: &TensorBinding,
    output: &TensorBinding,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let tensor_type = format!("tensor<?x?x?x?x{ty}>");
    let param_type = format!("tensor<?x{ty}>");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: BatchNormalization
module @{ep_name} {{
  func.func @main(%{inp}: {t}, %{sc}: {pt}, %{bi}: {pt}) -> {t} {{
    %{out} = stablehlo.batch_norm_inference %{inp}, %{sc}, %{bi}, %{sc}, %{bi},
      epsilon = 1.0e-5, feature_index = 1
      : ({t}, {pt}, {pt}, {pt}, {pt}) -> {t}
    return %{out} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        inp = input.name,
        sc = scale.name,
        bi = bias.name,
        out = output.name,
        t = tensor_type,
        pt = param_type,
    )
}

#[cfg(test)]
mod tests {
    use super::*;
    use nxpu_backend_onnx::analyze::{
        ActivationOp, MatMulShape, PoolKind, PoolShape, ReduceOp, TensorRole,
    };

    fn dummy_handle() -> nxpu_ir::Handle<nxpu_ir::GlobalVariable> {
        let mut arena = nxpu_ir::Arena::new();
        arena.append(nxpu_ir::GlobalVariable {
            name: None,
            space: nxpu_ir::AddressSpace::Uniform,
            binding: None,
            ty: {
                let mut types = nxpu_ir::UniqueArena::new();
                types.insert(nxpu_ir::Type {
                    name: None,
                    inner: nxpu_ir::TypeInner::Scalar(nxpu_ir::Scalar::F32),
                })
            },
            init: None,
        })
    }

    fn make_tensor(name: &str, role: TensorRole) -> TensorBinding {
        TensorBinding {
            handle: dummy_handle(),
            name: name.into(),
            elem_type: data_type::FLOAT,
            role,
        }
    }

    #[test]
    fn matmul_mlir() {
        let pattern = KernelPattern::MatMul {
            inputs: [
                make_tensor("A", TensorRole::Input),
                make_tensor("B", TensorRole::Input),
            ],
            output: make_tensor("C", TensorRole::Output),
            shape: MatMulShape {
                m: "M".into(),
                n: "N".into(),
                k: "K".into(),
            },
        };

        let mlir = build_mlir(&pattern, "matmul_kernel");
        assert!(mlir.contains("stablehlo.dot_general"));
        assert!(mlir.contains("module @matmul_kernel"));
        assert!(mlir.contains("%A"));
        assert!(mlir.contains("%B"));
        assert!(mlir.contains("%C"));
    }

    #[test]
    fn elementwise_add_mlir() {
        let pattern = KernelPattern::ElementWise {
            op: ElementWiseOp::Add,
            inputs: [
                make_tensor("x", TensorRole::Input),
                make_tensor("y", TensorRole::Input),
            ],
            output: make_tensor("z", TensorRole::Output),
            dim_name: "N".into(),
        };

        let mlir = build_mlir(&pattern, "vecadd");
        assert!(mlir.contains("stablehlo.add"));
        assert!(mlir.contains("module @vecadd"));
    }

    #[test]
    fn elementwise_ops() {
        for (op, expected) in [
            (ElementWiseOp::Sub, "stablehlo.subtract"),
            (ElementWiseOp::Mul, "stablehlo.multiply"),
            (ElementWiseOp::Div, "stablehlo.divide"),
        ] {
            let pattern = KernelPattern::ElementWise {
                op,
                inputs: [
                    make_tensor("a", TensorRole::Input),
                    make_tensor("b", TensorRole::Input),
                ],
                output: make_tensor("c", TensorRole::Output),
                dim_name: "N".into(),
            };
            let mlir = build_mlir(&pattern, "test");
            assert!(mlir.contains(expected), "missing {expected}");
        }
    }

    #[test]
    fn conv2d_mlir() {
        let pattern = KernelPattern::Conv2D {
            input: make_tensor("input", TensorRole::Input),
            weight: make_tensor("weight", TensorRole::Input),
            output: make_tensor("output", TensorRole::Output),
            shape: nxpu_backend_onnx::analyze::Conv2DShape {
                batch: "N".into(),
                channels_in: "IC".into(),
                channels_out: "OC".into(),
                height: "H".into(),
                width: "W".into(),
                kernel_h: "KH".into(),
                kernel_w: "KW".into(),
                stride_h: 1,
                stride_w: 1,
                pad_h: 0,
                pad_w: 0,
            },
        };
        let mlir = build_mlir(&pattern, "conv2d");
        assert!(mlir.contains("stablehlo.convolution"));
    }

    #[test]
    fn activation_relu_mlir() {
        let pattern = KernelPattern::Activation {
            op: ActivationOp::Relu,
            input: make_tensor("x", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            dim_name: "N".into(),
        };
        let mlir = build_mlir(&pattern, "relu");
        assert!(mlir.contains("stablehlo.maximum"));
    }

    #[test]
    fn activation_tanh_mlir() {
        let pattern = KernelPattern::Activation {
            op: ActivationOp::Tanh,
            input: make_tensor("x", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            dim_name: "N".into(),
        };
        let mlir = build_mlir(&pattern, "tanh_test");
        assert!(mlir.contains("stablehlo.tanh"));
    }

    #[test]
    fn pool_max_mlir() {
        let pattern = KernelPattern::Pool {
            kind: PoolKind::Max,
            input: make_tensor("x", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            shape: PoolShape {
                kernel_h: 2,
                kernel_w: 2,
                stride_h: 2,
                stride_w: 2,
            },
        };
        let mlir = build_mlir(&pattern, "maxpool");
        assert!(mlir.contains("stablehlo.reduce_window"));
    }

    #[test]
    fn reduce_sum_mlir() {
        let pattern = KernelPattern::Reduce {
            op: ReduceOp::Sum,
            input: make_tensor("x", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            axis: 1,
        };
        let mlir = build_mlir(&pattern, "reduce_sum");
        assert!(mlir.contains("stablehlo.reduce"));
    }

    #[test]
    fn transpose_mlir() {
        let pattern = KernelPattern::Transpose {
            input: make_tensor("x", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            perm: vec![1, 0],
        };
        let mlir = build_mlir(&pattern, "transpose");
        assert!(mlir.contains("stablehlo.transpose"));
    }

    #[test]
    fn normalization_mlir() {
        let pattern = KernelPattern::Normalization {
            input: make_tensor("x", TensorRole::Input),
            scale: make_tensor("gamma", TensorRole::Input),
            bias: make_tensor("beta", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            epsilon: 1e-5,
        };
        let mlir = build_mlir(&pattern, "batchnorm");
        assert!(mlir.contains("stablehlo.batch_norm_inference"));
    }
}
