//! StableHLO MLIR text emission from classified kernel patterns.
//!
//! Emits valid MLIR textual format with StableHLO dialect operations
//! targeting Google Cloud TPU via the OpenXLA compiler stack.

use nxpu_backend_onnx::analyze::{ElementWiseOp, KernelPattern, TensorBinding};
use nxpu_backend_onnx::proto::data_type;

/// Build StableHLO MLIR text from a classified kernel pattern.
pub fn build_mlir(pattern: &KernelPattern, ep_name: &str) -> String {
    match pattern {
        KernelPattern::MatMul {
            inputs,
            output,
            shape,
        } => build_matmul_mlir(&inputs[0], &inputs[1], output, shape, ep_name),
        KernelPattern::ElementWise {
            op,
            inputs,
            output,
            dim_name,
        } => build_elementwise_mlir(*op, &inputs[0], &inputs[1], output, dim_name, ep_name),
    }
}

fn onnx_to_mlir_type(onnx_dt: i32) -> &'static str {
    match onnx_dt {
        data_type::FLOAT => "f32",
        data_type::FLOAT16 => "f16",
        data_type::INT32 => "i32",
        data_type::UINT32 => "ui32",
        data_type::BOOL => "i1",
        _ => "f32",
    }
}

fn build_matmul_mlir(
    a: &TensorBinding,
    b: &TensorBinding,
    c: &TensorBinding,
    shape: &nxpu_backend_onnx::analyze::MatMulShape,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(a.elem_type);
    let a_type = format!("tensor<?x?x{ty}>");
    let b_type = format!("tensor<?x?x{ty}>");
    let c_type = format!("tensor<?x?x{ty}>");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: MatMul [{m},{k}] x [{k},{n}] -> [{m},{n}]
module @{ep_name} {{
  func.func @main(%{a}: {a_type}, %{b}: {b_type}) -> {c_type} {{
    %{c} = stablehlo.dot_general %{a}, %{b},
      batching_dims = [] x [],
      contracting_dims = [1] x [0]
      : ({a_type}, {b_type}) -> {c_type}
    return %{c} : {c_type}
  }}
}}
"#,
        ep_name = ep_name,
        m = shape.m,
        n = shape.n,
        k = shape.k,
        a = a.name,
        b = b.name,
        c = c.name,
        a_type = a_type,
        b_type = b_type,
        c_type = c_type,
    )
}

fn build_elementwise_mlir(
    op: ElementWiseOp,
    a: &TensorBinding,
    b: &TensorBinding,
    c: &TensorBinding,
    dim_name: &str,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(a.elem_type);
    let tensor_type = format!("tensor<?x{ty}>");

    let stablehlo_op = match op {
        ElementWiseOp::Add => "stablehlo.add",
        ElementWiseOp::Sub => "stablehlo.subtract",
        ElementWiseOp::Mul => "stablehlo.multiply",
        ElementWiseOp::Div => "stablehlo.divide",
    };

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: {op_name} [{dim}]
module @{ep_name} {{
  func.func @main(%{a}: {t}, %{b}: {t}) -> {t} {{
    %{c} = {stablehlo_op} %{a}, %{b} : {t}
    return %{c} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        op_name = op.onnx_op_type(),
        dim = dim_name,
        a = a.name,
        b = b.name,
        c = c.name,
        t = tensor_type,
        stablehlo_op = stablehlo_op,
    )
}

#[cfg(test)]
mod tests {
    use super::*;
    use nxpu_backend_onnx::analyze::{MatMulShape, TensorRole};

    fn dummy_handle() -> nxpu_ir::Handle<nxpu_ir::GlobalVariable> {
        let mut arena = nxpu_ir::Arena::new();
        arena.append(nxpu_ir::GlobalVariable {
            name: None,
            space: nxpu_ir::AddressSpace::Uniform,
            binding: None,
            ty: {
                let mut types = nxpu_ir::UniqueArena::new();
                types.insert(nxpu_ir::Type {
                    name: None,
                    inner: nxpu_ir::TypeInner::Scalar(nxpu_ir::Scalar::F32),
                })
            },
            init: None,
        })
    }

    fn make_tensor(name: &str, role: TensorRole) -> TensorBinding {
        TensorBinding {
            handle: dummy_handle(),
            name: name.into(),
            elem_type: data_type::FLOAT,
            role,
        }
    }

    #[test]
    fn matmul_mlir() {
        let pattern = KernelPattern::MatMul {
            inputs: [
                make_tensor("A", TensorRole::Input),
                make_tensor("B", TensorRole::Input),
            ],
            output: make_tensor("C", TensorRole::Output),
            shape: MatMulShape {
                m: "M".into(),
                n: "N".into(),
                k: "K".into(),
            },
        };

        let mlir = build_mlir(&pattern, "matmul_kernel");
        assert!(mlir.contains("stablehlo.dot_general"));
        assert!(mlir.contains("module @matmul_kernel"));
        assert!(mlir.contains("%A"));
        assert!(mlir.contains("%B"));
        assert!(mlir.contains("%C"));
    }

    #[test]
    fn elementwise_add_mlir() {
        let pattern = KernelPattern::ElementWise {
            op: ElementWiseOp::Add,
            inputs: [
                make_tensor("x", TensorRole::Input),
                make_tensor("y", TensorRole::Input),
            ],
            output: make_tensor("z", TensorRole::Output),
            dim_name: "N".into(),
        };

        let mlir = build_mlir(&pattern, "vecadd");
        assert!(mlir.contains("stablehlo.add"));
        assert!(mlir.contains("module @vecadd"));
    }

    #[test]
    fn elementwise_ops() {
        for (op, expected) in [
            (ElementWiseOp::Sub, "stablehlo.subtract"),
            (ElementWiseOp::Mul, "stablehlo.multiply"),
            (ElementWiseOp::Div, "stablehlo.divide"),
        ] {
            let pattern = KernelPattern::ElementWise {
                op,
                inputs: [
                    make_tensor("a", TensorRole::Input),
                    make_tensor("b", TensorRole::Input),
                ],
                output: make_tensor("c", TensorRole::Output),
                dim_name: "N".into(),
            };
            let mlir = build_mlir(&pattern, "test");
            assert!(mlir.contains(expected), "missing {expected}");
        }
    }
}
