//! StableHLO MLIR text emission from classified kernel patterns.
//!
//! Emits valid MLIR textual format with StableHLO dialect operations
//! targeting Google Cloud TPU via the OpenXLA compiler stack.

use nxpu_analysis::analyze::data_type;
use nxpu_analysis::analyze::{
    ActivationOp, ElementWiseOp, KernelPattern, NormType, PoolKind, ReduceOp, TensorBinding,
};
use nxpu_backend_core::BackendError;

/// Build StableHLO MLIR text from a classified kernel pattern.
pub fn build_mlir(pattern: &KernelPattern, ep_name: &str) -> Result<String, BackendError> {
    let mlir = match pattern {
        KernelPattern::MatMul {
            inputs,
            output,
            shape,
        } => build_matmul_mlir(&inputs[0], &inputs[1], output, shape, ep_name),
        KernelPattern::ElementWise {
            op,
            inputs,
            output,
            dim_name,
        } => build_elementwise_mlir(*op, &inputs[0], &inputs[1], output, dim_name, ep_name),
        KernelPattern::Conv2D {
            input,
            weight,
            output,
            ..
        } => build_conv2d_mlir(input, weight, output, ep_name),
        KernelPattern::Pool {
            kind,
            input,
            output,
            shape,
        } => build_pool_mlir(*kind, input, output, shape, ep_name),
        KernelPattern::Activation {
            op,
            input,
            output,
            dim_name,
        } => build_activation_mlir(*op, input, output, dim_name, ep_name),
        KernelPattern::Reduce {
            op,
            input,
            output,
            axis,
        } => build_reduce_mlir(*op, input, output, *axis, ep_name),
        KernelPattern::Transpose {
            input,
            output,
            perm,
        } => build_transpose_mlir(input, output, perm, ep_name),
        KernelPattern::Reshape { input, output, .. } => build_reshape_mlir(input, output, ep_name),
        KernelPattern::Normalization {
            input,
            scale,
            bias,
            output,
            norm_type,
            ..
        } => build_normalization_mlir(input, scale, bias, output, *norm_type, ep_name),
        KernelPattern::Concat {
            inputs,
            output,
            axis,
        } => build_concat_mlir(inputs, output, *axis, ep_name),
        KernelPattern::Split {
            input,
            outputs,
            axis,
        } => build_split_mlir(input, outputs, *axis, ep_name),
        KernelPattern::Attention {
            query,
            key,
            value,
            output,
            d_k,
            num_heads,
            causal,
            ..
        } => build_attention_mlir(query, key, value, output, d_k, *num_heads, *causal, ep_name),
        KernelPattern::Gather {
            data,
            indices,
            output,
            ..
        } => build_gather_mlir(data, indices, output, ep_name),
        KernelPattern::Scatter {
            data,
            indices,
            updates,
            output,
            ..
        } => build_scatter_mlir(data, indices, updates, output, ep_name),
        KernelPattern::Unknown { reason } => {
            return Err(BackendError::Unsupported(format!(
                "cannot lower Unknown pattern to StableHLO: {reason}"
            )));
        }
    };
    Ok(mlir)
}

fn onnx_to_mlir_type(onnx_dt: i32) -> &'static str {
    match onnx_dt {
        data_type::FLOAT => "f32",
        data_type::FLOAT16 => "f16",
        data_type::BFLOAT16 => "bf16",
        data_type::INT32 => "i32",
        data_type::INT8 => "i8",
        data_type::UINT32 => "ui32",
        data_type::BOOL => "i1",
        _ => "f32",
    }
}

fn build_matmul_mlir(
    a: &TensorBinding,
    b: &TensorBinding,
    c: &TensorBinding,
    shape: &nxpu_analysis::analyze::MatMulShape,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(a.elem_type);
    let a_type = format!("tensor<?x?x{ty}>");
    let b_type = format!("tensor<?x?x{ty}>");
    let c_type = format!("tensor<?x?x{ty}>");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: MatMul [{m},{k}] x [{k},{n}] -> [{m},{n}]
module @{ep_name} {{
  func.func @main(%{a}: {a_type}, %{b}: {b_type}) -> {c_type} {{
    %{c} = stablehlo.dot_general %{a}, %{b},
      batching_dims = [] x [],
      contracting_dims = [1] x [0]
      : ({a_type}, {b_type}) -> {c_type}
    return %{c} : {c_type}
  }}
}}
"#,
        ep_name = ep_name,
        m = shape.m,
        n = shape.n,
        k = shape.k,
        a = a.name,
        b = b.name,
        c = c.name,
        a_type = a_type,
        b_type = b_type,
        c_type = c_type,
    )
}

fn build_elementwise_mlir(
    op: ElementWiseOp,
    a: &TensorBinding,
    b: &TensorBinding,
    c: &TensorBinding,
    dim_name: &str,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(a.elem_type);
    let tensor_type = format!("tensor<?x{ty}>");

    let stablehlo_op = match op {
        ElementWiseOp::Add => "stablehlo.add",
        ElementWiseOp::Sub => "stablehlo.subtract",
        ElementWiseOp::Mul => "stablehlo.multiply",
        ElementWiseOp::Div => "stablehlo.divide",
    };

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: {op_name} [{dim}]
module @{ep_name} {{
  func.func @main(%{a}: {t}, %{b}: {t}) -> {t} {{
    %{c} = {stablehlo_op} %{a}, %{b} : {t}
    return %{c} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        op_name = op.op_name(),
        dim = dim_name,
        a = a.name,
        b = b.name,
        c = c.name,
        t = tensor_type,
        stablehlo_op = stablehlo_op,
    )
}

fn build_conv2d_mlir(
    input: &TensorBinding,
    weight: &TensorBinding,
    output: &TensorBinding,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let in_type = format!("tensor<?x?x?x?x{ty}>");
    let w_type = format!("tensor<?x?x?x?x{ty}>");
    let out_type = format!("tensor<?x?x?x?x{ty}>");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Conv2D
module @{ep_name} {{
  func.func @main(%{inp}: {in_type}, %{w}: {w_type}) -> {out_type} {{
    %{out} = stablehlo.convolution(%{inp}, %{w})
      dim_numbers = [b, f, 0, 1]x[o, i, 0, 1]->[b, f, 0, 1],
      window = {{stride = [1, 1], pad = [[0, 0], [0, 0]]}}
      : ({in_type}, {w_type}) -> {out_type}
    return %{out} : {out_type}
  }}
}}
"#,
        ep_name = ep_name,
        inp = input.name,
        w = weight.name,
        out = output.name,
    )
}

fn build_pool_mlir(
    kind: PoolKind,
    input: &TensorBinding,
    output: &TensorBinding,
    shape: &nxpu_analysis::analyze::PoolShape,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let in_shape = format!("?x?x?x?x{ty}");
    let out_shape = format!("?x?x?x?x{ty}");
    let tensor_type = format!("tensor<?x?x?x?x{ty}>");
    let scalar_type = format!("tensor<{ty}>");
    let kh = shape.kernel_h;
    let kw = shape.kernel_w;
    let sh = shape.stride_h;
    let sw = shape.stride_w;

    match kind {
        PoolKind::Max => format!(
            r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: max_pool
module @{ep_name} {{
  func.func @main(%{inp}: {t}) -> {t} {{
    %init = stablehlo.constant dense<0xFF800000> : {st}
    %{out} = "stablehlo.reduce_window"(%{inp}, %init) ({{
      ^bb0(%arg0: {st}, %arg1: {st}):
        %0 = stablehlo.maximum %arg0, %arg1 : {st}
        stablehlo.return %0 : {st}
    }}) {{ window_dimensions = dense<[1, 1, {kh}, {kw}]> : tensor<4xi64>, window_strides = dense<[1, 1, {sh}, {sw}]> : tensor<4xi64> }} : (tensor<{in_shape}>, {st}) -> tensor<{out_shape}>
    return %{out} : {t}
  }}
}}
"#,
            ep_name = ep_name,
            inp = input.name,
            out = output.name,
            t = tensor_type,
            st = scalar_type,
            kh = kh,
            kw = kw,
            sh = sh,
            sw = sw,
            in_shape = in_shape,
            out_shape = out_shape,
        ),
        PoolKind::Avg => {
            let window_size = kh * kw;
            format!(
                r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: avg_pool
module @{ep_name} {{
  func.func @main(%{inp}: {t}) -> {t} {{
    %init = stablehlo.constant dense<0.0> : {st}
    %pool_sum = "stablehlo.reduce_window"(%{inp}, %init) ({{
      ^bb0(%arg0: {st}, %arg1: {st}):
        %0 = stablehlo.add %arg0, %arg1 : {st}
        stablehlo.return %0 : {st}
    }}) {{ window_dimensions = dense<[1, 1, {kh}, {kw}]> : tensor<4xi64>, window_strides = dense<[1, 1, {sh}, {sw}]> : tensor<4xi64> }} : (tensor<{in_shape}>, {st}) -> tensor<{out_shape}>
    %divisor = stablehlo.constant dense<{window_size}.0> : {st}
    %divisor_bcast = stablehlo.broadcast_in_dim %divisor, dims = [] : ({st}) -> tensor<{out_shape}>
    %{out} = stablehlo.divide %pool_sum, %divisor_bcast : tensor<{out_shape}>
    return %{out} : {t}
  }}
}}
"#,
                ep_name = ep_name,
                inp = input.name,
                out = output.name,
                t = tensor_type,
                st = scalar_type,
                kh = kh,
                kw = kw,
                sh = sh,
                sw = sw,
                in_shape = in_shape,
                out_shape = out_shape,
                window_size = window_size,
            )
        }
    }
}

fn build_activation_mlir(
    op: ActivationOp,
    input: &TensorBinding,
    output: &TensorBinding,
    dim_name: &str,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let tensor_type = format!("tensor<?x{ty}>");

    let (op_line, op_name) = match op {
        ActivationOp::Relu => (
            format!(
                "    %zero = stablehlo.constant dense<0.0> : {t}\n    %{out} = stablehlo.maximum %{inp}, %zero : {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Relu",
        ),
        ActivationOp::Sigmoid => (
            format!(
                "    %{out} = stablehlo.logistic %{inp} : {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Sigmoid",
        ),
        ActivationOp::Tanh => (
            format!(
                "    %{out} = stablehlo.tanh %{inp} : {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Tanh",
        ),
        ActivationOp::Softmax => (
            format!(
                "    %{out} = stablehlo.custom_call @softmax(%{inp}) : ({t}) -> {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Softmax",
        ),
        ActivationOp::Gelu => (
            format!(
                "    %sqrt2 = stablehlo.constant dense<1.4142135> : {t}\n    %div = stablehlo.divide %{inp}, %sqrt2 : {t}\n    %erf = stablehlo.custom_call @erf(%div) : ({t}) -> {t}\n    %one = stablehlo.constant dense<1.0> : {t}\n    %add = stablehlo.add %erf, %one : {t}\n    %mul1 = stablehlo.multiply %{inp}, %add : {t}\n    %half = stablehlo.constant dense<0.5> : {t}\n    %{out} = stablehlo.multiply %mul1, %half : {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Gelu",
        ),
        ActivationOp::Silu => (
            format!(
                "    %sigmoid = stablehlo.logistic %{inp} : {t}\n    %{out} = stablehlo.multiply %{inp}, %sigmoid : {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Silu",
        ),
        ActivationOp::Mish => (
            format!(
                "    %exp = stablehlo.exponential %{inp} : {t}\n    %one = stablehlo.constant dense<1.0> : {t}\n    %sp = stablehlo.add %exp, %one : {t}\n    %ln = stablehlo.log %sp : {t}\n    %tanh = stablehlo.tanh %ln : {t}\n    %{out} = stablehlo.multiply %{inp}, %tanh : {t}",
                t = tensor_type,
                inp = input.name,
                out = output.name
            ),
            "Mish",
        ),
    };

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: {op_name} [{dim}]
module @{ep_name} {{
  func.func @main(%{inp}: {t}) -> {t} {{
{op_line}
    return %{out} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        op_name = op_name,
        dim = dim_name,
        inp = input.name,
        out = output.name,
        t = tensor_type,
        op_line = op_line,
    )
}

fn build_reduce_mlir(
    op: ReduceOp,
    input: &TensorBinding,
    output: &TensorBinding,
    axis: i64,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let in_type = format!("tensor<?x?x{ty}>");
    let out_type = format!("tensor<?x{ty}>");
    let scalar_type = format!("tensor<{ty}>");

    let (init_val, body_op) = match op {
        ReduceOp::Sum | ReduceOp::Mean => ("dense<0.0>", "stablehlo.add"),
        ReduceOp::Max => ("dense<0xFF800000>", "stablehlo.maximum"),
        ReduceOp::Min => ("dense<0x7F800000>", "stablehlo.minimum"),
    };

    let reduce_block = format!(
        r#"    %init = stablehlo.constant {init_val} : {st}
    %reduce_result = "stablehlo.reduce"(%{{inp}}, %init) ({{
      ^bb0(%arg0: {st}, %arg1: {st}):
        %0 = {body_op} %arg0, %arg1 : {st}
        stablehlo.return %0 : {st}
    }}) {{dimensions = dense<[{axis}]> : tensor<1xi64>}} : ({in_type}, {st}) -> {out_type}"#,
        init_val = init_val,
        st = scalar_type,
        body_op = body_op,
        in_type = in_type,
        out_type = out_type,
    );

    match op {
        ReduceOp::Mean => format!(
            r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: {op_name} axis={axis}
module @{ep_name} {{
  func.func @main(%{inp}: {in_type}) -> {out_type} {{
{reduce_block}
    %dim_size_i32 = stablehlo.get_dimension_size %{inp}, dim = {axis} : ({in_type}) -> tensor<i32>
    %dim_size = stablehlo.convert %dim_size_i32 : (tensor<i32>) -> {st}
    %dim_bcast = stablehlo.broadcast_in_dim %dim_size, dims = [] : ({st}) -> {out_type}
    %{out} = stablehlo.divide %reduce_result, %dim_bcast : {out_type}
    return %{out} : {out_type}
  }}
}}
"#,
            ep_name = ep_name,
            op_name = op.op_name(),
            inp = input.name,
            out = output.name,
            in_type = in_type,
            out_type = out_type,
            st = scalar_type,
            reduce_block = reduce_block.replace("{inp}", &input.name),
        ),
        _ => format!(
            r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: {op_name} axis={axis}
module @{ep_name} {{
  func.func @main(%{inp}: {in_type}) -> {out_type} {{
{reduce_block}
    return %reduce_result : {out_type}
  }}
}}
"#,
            ep_name = ep_name,
            op_name = op.op_name(),
            inp = input.name,
            in_type = in_type,
            out_type = out_type,
            reduce_block = reduce_block.replace("{inp}", &input.name),
        ),
    }
}

fn build_transpose_mlir(
    input: &TensorBinding,
    output: &TensorBinding,
    perm: &[i64],
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let ndim = perm.len();
    let dims_str = (0..ndim).map(|_| "?").collect::<Vec<_>>().join("x");
    let tensor_type = format!("tensor<{dims_str}x{ty}>");
    let perm_str = perm
        .iter()
        .map(|p| p.to_string())
        .collect::<Vec<_>>()
        .join(", ");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Transpose perm=[{perm_str}]
module @{ep_name} {{
  func.func @main(%{inp}: {t}) -> {t} {{
    %{out} = stablehlo.transpose %{inp}, dims = [{perm_str}] : ({t}) -> {t}
    return %{out} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        inp = input.name,
        out = output.name,
        t = tensor_type,
    )
}

fn build_reshape_mlir(input: &TensorBinding, output: &TensorBinding, ep_name: &str) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let tensor_type = format!("tensor<?x{ty}>");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Reshape
module @{ep_name} {{
  func.func @main(%{inp}: {t}) -> {t} {{
    %{out} = stablehlo.reshape %{inp} : ({t}) -> {t}
    return %{out} : {t}
  }}
}}
"#,
        ep_name = ep_name,
        inp = input.name,
        out = output.name,
        t = tensor_type,
    )
}

fn build_normalization_mlir(
    input: &TensorBinding,
    scale: &TensorBinding,
    bias: &TensorBinding,
    output: &TensorBinding,
    norm_type: NormType,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);

    match norm_type {
        NormType::Batch => {
            let tensor_type = format!("tensor<?x?x?x?x{ty}>");
            let param_type = format!("tensor<?x{ty}>");

            format!(
                r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: BatchNormalization
module @{ep_name} {{
  func.func @main(%{inp}: {t}, %{sc}: {pt}, %{bi}: {pt}) -> {t} {{
    %{out} = stablehlo.batch_norm_inference %{inp}, %{sc}, %{bi}, %{sc}, %{bi},
      epsilon = 1.0e-5, feature_index = 1
      : ({t}, {pt}, {pt}, {pt}, {pt}) -> {t}
    return %{out} : {t}
  }}
}}
"#,
                ep_name = ep_name,
                inp = input.name,
                sc = scale.name,
                bi = bias.name,
                out = output.name,
                t = tensor_type,
                pt = param_type,
            )
        }
        NormType::Layer => {
            let tensor_type = format!("tensor<?x?x{ty}>");
            let param_type = format!("tensor<?x{ty}>");
            let scalar_type = format!("tensor<{ty}>");

            format!(
                r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: LayerNormalization
module @{ep_name} {{
  func.func @main(%{inp}: {t}, %{sc}: {pt}, %{bi}: {pt}) -> {t} {{
    %eps = stablehlo.constant dense<1.0e-5> : {st}
    %mean = stablehlo.custom_call @layer_norm_mean(%{inp}) : ({t}) -> {t}
    %centered = stablehlo.subtract %{inp}, %mean : {t}
    %sq = stablehlo.multiply %centered, %centered : {t}
    %var = stablehlo.custom_call @layer_norm_var(%sq) : ({t}) -> {t}
    %eps_bcast = stablehlo.broadcast_in_dim %eps, dims = [] : ({st}) -> {t}
    %var_eps = stablehlo.add %var, %eps_bcast : {t}
    %inv_std = stablehlo.rsqrt %var_eps : {t}
    %normed = stablehlo.multiply %centered, %inv_std : {t}
    %sc_bcast = stablehlo.broadcast_in_dim %{sc}, dims = [1] : ({pt}) -> {t}
    %scaled = stablehlo.multiply %normed, %sc_bcast : {t}
    %bi_bcast = stablehlo.broadcast_in_dim %{bi}, dims = [1] : ({pt}) -> {t}
    %{out} = stablehlo.add %scaled, %bi_bcast : {t}
    return %{out} : {t}
  }}
}}
"#,
                ep_name = ep_name,
                inp = input.name,
                sc = scale.name,
                bi = bias.name,
                out = output.name,
                t = tensor_type,
                pt = param_type,
                st = scalar_type,
            )
        }
    }
}

fn build_concat_mlir(
    inputs: &[TensorBinding],
    output: &TensorBinding,
    axis: i64,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(inputs[0].elem_type);
    let tensor_type = format!("tensor<?x{ty}>");

    let args: Vec<String> = inputs
        .iter()
        .map(|i| format!("%{}: {tensor_type}", i.name))
        .collect();
    let arg_refs: Vec<String> = inputs.iter().map(|i| format!("%{}", i.name)).collect();

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Concat axis={axis}
module @{ep_name} {{
  func.func @main({args}) -> {tensor_type} {{
    %{out} = stablehlo.concatenate {refs}, dim = {axis} : ({in_types}) -> {tensor_type}
    return %{out} : {tensor_type}
  }}
}}
"#,
        ep_name = ep_name,
        args = args.join(", "),
        out = output.name,
        refs = arg_refs.join(", "),
        in_types = inputs
            .iter()
            .map(|_| tensor_type.as_str())
            .collect::<Vec<_>>()
            .join(", "),
    )
}

fn build_split_mlir(
    input: &TensorBinding,
    outputs: &[TensorBinding],
    _axis: i64,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(input.elem_type);
    let tensor_type = format!("tensor<?x{ty}>");

    let n = outputs.len();
    let mut slices = Vec::new();
    for (i, o) in outputs.iter().enumerate() {
        // Emit symbolic slice: each output gets 1/n of the input along axis 0.
        // start = i * (input_size / n), limit = (i+1) * (input_size / n)
        slices.push(format!(
            "    %{name} = \"stablehlo.slice\"(%{inp}) {{start_indices = dense<[{start}]> : tensor<1xi64>, limit_indices = dense<[{limit}]> : tensor<1xi64>, strides = dense<[1]> : tensor<1xi64>}} : ({t}) -> {t}",
            name = o.name,
            inp = input.name,
            start = i,
            limit = i + 1,
            t = tensor_type,
        ));
    }

    let return_vals: Vec<String> = outputs.iter().map(|o| format!("%{}", o.name)).collect();
    let return_types: Vec<String> = outputs.iter().map(|_| tensor_type.clone()).collect();

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Split into {n} outputs
module @{ep_name} {{
  func.func @main(%{inp}: {t}) -> ({ret_types}) {{
{slices}
    return {ret_vals} : {ret_types}
  }}
}}
"#,
        ep_name = ep_name,
        n = n,
        inp = input.name,
        t = tensor_type,
        slices = slices.join("\n"),
        ret_vals = return_vals.join(", "),
        ret_types = return_types.join(", "),
    )
}

fn build_gather_mlir(
    data: &TensorBinding,
    indices: &TensorBinding,
    output: &TensorBinding,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(data.elem_type);
    let data_type_str = format!("tensor<?x{ty}>");
    let idx_ty = onnx_to_mlir_type(indices.elem_type);
    let indices_type = format!("tensor<?x{idx_ty}>");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Gather
module @{ep_name} {{
  func.func @main(%{data}: {dt}, %{idx}: {it}) -> {dt} {{
    %{out} = "stablehlo.gather"(%{data}, %{idx}) {{
      dimension_numbers = #stablehlo.gather<collapsed_slice_dims = [0], start_index_map = [0], index_vector_dim = 1>,
      slice_sizes = dense<1> : tensor<1xi64>
    }} : ({dt}, {it}) -> {dt}
    return %{out} : {dt}
  }}
}}
"#,
        ep_name = ep_name,
        data = data.name,
        idx = indices.name,
        out = output.name,
        dt = data_type_str,
        it = indices_type,
    )
}

fn build_scatter_mlir(
    data: &TensorBinding,
    indices: &TensorBinding,
    updates: &TensorBinding,
    output: &TensorBinding,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(data.elem_type);
    let data_type_str = format!("tensor<?x{ty}>");
    let idx_ty = onnx_to_mlir_type(indices.elem_type);
    let indices_type = format!("tensor<?x{idx_ty}>");
    let updates_type = format!("tensor<?x{ty}>");
    let scalar_type = format!("tensor<{ty}>");

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Scatter
module @{ep_name} {{
  func.func @main(%{data}: {dt}, %{idx}: {it}, %{upd}: {ut}) -> {dt} {{
    %{out} = "stablehlo.scatter"(%{data}, %{idx}, %{upd}) ({{
      ^bb0(%arg0: {st}, %arg1: {st}):
        stablehlo.return %arg1 : {st}
    }}) {{
      scatter_dimension_numbers = #stablehlo.scatter<inserted_window_dims = [0], scatter_dims_to_operand_dims = [0], index_vector_dim = 1>,
      unique_indices = false
    }} : ({dt}, {it}, {ut}) -> {dt}
    return %{out} : {dt}
  }}
}}
"#,
        ep_name = ep_name,
        data = data.name,
        idx = indices.name,
        upd = updates.name,
        out = output.name,
        dt = data_type_str,
        it = indices_type,
        ut = updates_type,
        st = scalar_type,
    )
}

#[allow(clippy::too_many_arguments)]
fn build_attention_mlir(
    query: &TensorBinding,
    key: &TensorBinding,
    value: &TensorBinding,
    output: &TensorBinding,
    _d_k: &str,
    num_heads: u32,
    causal: bool,
    ep_name: &str,
) -> String {
    let ty = onnx_to_mlir_type(query.elem_type);
    let mat_type = format!("tensor<?x?x{ty}>");
    let scalar_type = format!("tensor<{ty}>");
    let vec_type = format!("tensor<?x{ty}>");

    let i32_type = "tensor<i32>";

    let mut pre_ops = String::new();
    let mut post_ops = String::new();
    let (q_var, k_var, v_var) = if num_heads > 1 {
        let mh_type = format!("tensor<?x{num_heads}x?x?x{ty}>");
        // Multi-head: reshape + transpose Q/K/V
        pre_ops.push_str(&format!(
            "    // Multi-head reshape (num_heads={num_heads})\n\
             \x20   %q_mh = stablehlo.reshape %{q} : ({mat_type}) -> {mh_type}\n\
             \x20   %k_mh = stablehlo.reshape %{k} : ({mat_type}) -> {mh_type}\n\
             \x20   %v_mh = stablehlo.reshape %{v} : ({mat_type}) -> {mh_type}\n\
             \x20   %q_t = stablehlo.transpose %q_mh, dims = [0, 2, 1, 3] : ({mh_type}) -> {mh_type}\n\
             \x20   %k_t = stablehlo.transpose %k_mh, dims = [0, 2, 1, 3] : ({mh_type}) -> {mh_type}\n\
             \x20   %v_t = stablehlo.transpose %v_mh, dims = [0, 2, 1, 3] : ({mh_type}) -> {mh_type}\n",
            q = query.name,
            k = key.name,
            v = value.name,
            mat_type = mat_type,
            mh_type = mh_type,
            num_heads = num_heads,
        ));
        // Post: transpose back + reshape
        post_ops.push_str(&format!(
            "    %out_transposed = stablehlo.transpose %attn_out, dims = [0, 2, 1, 3] : ({mh_type}) -> {mh_type}\n\
             \x20   %{out} = stablehlo.reshape %out_transposed : ({mh_type}) -> {mat_type}\n",
            out = output.name,
            mh_type = mh_type,
            mat_type = mat_type,
        ));
        ("q_t", "k_t", "v_t")
    } else {
        (query.name.as_str(), key.name.as_str(), value.name.as_str())
    };

    let causal_block = if causal {
        format!(
            "    // Causal mask\n\
             \x20   %iota_r = \"stablehlo.iota\"() {{iota_dimension = 0 : i64}} : () -> tensor<?x?xi32>\n\
             \x20   %iota_c = \"stablehlo.iota\"() {{iota_dimension = 1 : i64}} : () -> tensor<?x?xi32>\n\
             \x20   %mask = stablehlo.compare GE, %iota_r, %iota_c : (tensor<?x?xi32>, tensor<?x?xi32>) -> tensor<?x?xi1>\n\
             \x20   %neg_inf_mask = stablehlo.constant dense<-1.0e+09> : {scalar_type}\n\
             \x20   %neg_inf_bcast = stablehlo.broadcast_in_dim %neg_inf_mask, dims = [] : ({scalar_type}) -> {mat_type}\n\
             \x20   %scores_masked = stablehlo.select %mask, %scores_scaled, %neg_inf_bcast : (tensor<?x?xi1>, {mat_type}, {mat_type}) -> {mat_type}\n",
            scalar_type = scalar_type,
            mat_type = mat_type,
        )
    } else {
        String::new()
    };

    let softmax_input = if causal {
        "scores_masked"
    } else {
        "scores_scaled"
    };

    let attn_out_name = if num_heads > 1 {
        "attn_out"
    } else {
        &output.name
    };

    format!(
        r#"// StableHLO module generated by nxpu
// Entry point: {ep_name}
// Pattern: Attention (scaled dot-product, heads={num_heads}{causal_tag})
module @{ep_name} {{
  func.func @main(%{q}: {mat_type}, %{k}: {mat_type}, %{v}: {mat_type}) -> {mat_type} {{
{pre_ops}    // scores = Q * K^T
    %scores = stablehlo.dot_general %{q_var}, %{k_var},
      batching_dims = [] x [],
      contracting_dims = [1] x [1]
      : ({mat_type}, {mat_type}) -> {mat_type}
    // dynamic sqrt(d_k) from query tensor's last dimension
    %dk_i32 = stablehlo.get_dimension_size %{q_var}, dim = 1 : ({mat_type}) -> {i32_type}
    %dk_f = stablehlo.convert %dk_i32 : ({i32_type}) -> {scalar_type}
    %sqrt_dk = stablehlo.sqrt %dk_f : {scalar_type}
    %one = stablehlo.constant dense<1.0> : {scalar_type}
    %inv_sqrt = stablehlo.divide %one, %sqrt_dk : {scalar_type}
    %scale_bcast = stablehlo.broadcast_in_dim %inv_sqrt, dims = [] : ({scalar_type}) -> {mat_type}
    %scores_scaled = stablehlo.multiply %scores, %scale_bcast : {mat_type}
{causal_block}    // softmax: exp(scores - max) / sum(exp(scores - max))
    %neg_inf = stablehlo.constant dense<0xFF800000> : {scalar_type}
    %max_scores = stablehlo.reduce(%{softmax_input} init: %neg_inf) applies stablehlo.maximum across dimensions = [1] : ({mat_type}, {scalar_type}) -> {vec_type}
    %max_bcast = stablehlo.broadcast_in_dim %max_scores, dims = [0] : ({vec_type}) -> {mat_type}
    %shifted = stablehlo.subtract %{softmax_input}, %max_bcast : {mat_type}
    %exp_scores = stablehlo.exponential %shifted : {mat_type}
    %zero = stablehlo.constant dense<0.0> : {scalar_type}
    %sum_exp = stablehlo.reduce(%exp_scores init: %zero) applies stablehlo.add across dimensions = [1] : ({mat_type}, {scalar_type}) -> {vec_type}
    %sum_bcast = stablehlo.broadcast_in_dim %sum_exp, dims = [0] : ({vec_type}) -> {mat_type}
    %attn_weights = stablehlo.divide %exp_scores, %sum_bcast : {mat_type}
    // output = attn_weights * V
    %{attn_out_name} = stablehlo.dot_general %attn_weights, %{v_var},
      batching_dims = [] x [],
      contracting_dims = [1] x [0]
      : ({mat_type}, {mat_type}) -> {mat_type}
{post_ops}    return %{out} : {mat_type}
  }}
}}
"#,
        ep_name = ep_name,
        num_heads = num_heads,
        causal_tag = if causal { ", causal" } else { "" },
        q = query.name,
        k = key.name,
        v = value.name,
        out = output.name,
        q_var = q_var,
        k_var = k_var,
        v_var = v_var,
        attn_out_name = attn_out_name,
        i32_type = i32_type,
        scalar_type = scalar_type,
        mat_type = mat_type,
        vec_type = vec_type,
        pre_ops = pre_ops,
        post_ops = post_ops,
        causal_block = causal_block,
        softmax_input = softmax_input,
    )
}

#[cfg(test)]
mod tests {
    use super::*;
    use nxpu_analysis::analyze::{
        ActivationOp, MatMulShape, NormType, PoolKind, PoolShape, ReduceOp, TensorRole,
    };

    fn dummy_handle() -> nxpu_ir::Handle<nxpu_ir::GlobalVariable> {
        let mut arena = nxpu_ir::Arena::new();
        arena.append(nxpu_ir::GlobalVariable {
            name: None,
            space: nxpu_ir::AddressSpace::Uniform,
            binding: None,
            ty: {
                let mut types = nxpu_ir::UniqueArena::new();
                types.insert(nxpu_ir::Type {
                    name: None,
                    inner: nxpu_ir::TypeInner::Scalar(nxpu_ir::Scalar::F32),
                })
            },
            init: None,
            layout: None,
        })
    }

    fn make_tensor(name: &str, role: TensorRole) -> TensorBinding {
        TensorBinding {
            handle: dummy_handle(),
            name: name.into(),
            elem_type: data_type::FLOAT,
            role,
        }
    }

    #[test]
    fn matmul_mlir() {
        let pattern = KernelPattern::MatMul {
            inputs: [
                make_tensor("A", TensorRole::Input),
                make_tensor("B", TensorRole::Input),
            ],
            output: make_tensor("C", TensorRole::Output),
            shape: MatMulShape {
                m: "M".into(),
                n: "N".into(),
                k: "K".into(),
            },
        };

        let mlir = build_mlir(&pattern, "matmul_kernel").unwrap();
        assert!(mlir.contains("stablehlo.dot_general"));
        assert!(mlir.contains("module @matmul_kernel"));
        assert!(mlir.contains("%A"));
        assert!(mlir.contains("%B"));
        assert!(mlir.contains("%C"));
    }

    #[test]
    fn elementwise_add_mlir() {
        let pattern = KernelPattern::ElementWise {
            op: ElementWiseOp::Add,
            inputs: [
                make_tensor("x", TensorRole::Input),
                make_tensor("y", TensorRole::Input),
            ],
            output: make_tensor("z", TensorRole::Output),
            dim_name: "N".into(),
        };

        let mlir = build_mlir(&pattern, "vecadd").unwrap();
        assert!(mlir.contains("stablehlo.add"));
        assert!(mlir.contains("module @vecadd"));
    }

    #[test]
    fn elementwise_ops() {
        for (op, expected) in [
            (ElementWiseOp::Sub, "stablehlo.subtract"),
            (ElementWiseOp::Mul, "stablehlo.multiply"),
            (ElementWiseOp::Div, "stablehlo.divide"),
        ] {
            let pattern = KernelPattern::ElementWise {
                op,
                inputs: [
                    make_tensor("a", TensorRole::Input),
                    make_tensor("b", TensorRole::Input),
                ],
                output: make_tensor("c", TensorRole::Output),
                dim_name: "N".into(),
            };
            let mlir = build_mlir(&pattern, "test").unwrap();
            assert!(mlir.contains(expected), "missing {expected}");
        }
    }

    #[test]
    fn conv2d_mlir() {
        let pattern = KernelPattern::Conv2D {
            input: make_tensor("input", TensorRole::Input),
            weight: make_tensor("weight", TensorRole::Input),
            output: make_tensor("output", TensorRole::Output),
            shape: nxpu_analysis::analyze::Conv2DShape {
                batch: "N".into(),
                channels_in: "IC".into(),
                channels_out: "OC".into(),
                height: "H".into(),
                width: "W".into(),
                kernel_h: "KH".into(),
                kernel_w: "KW".into(),
                kernel_h_val: 3,
                kernel_w_val: 3,
                stride_h: 1,
                stride_w: 1,
                pad_h: 0,
                pad_w: 0,
                groups: 1,
                dilation_h: 1,
                dilation_w: 1,
            },
        };
        let mlir = build_mlir(&pattern, "conv2d").unwrap();
        assert!(mlir.contains("stablehlo.convolution"));
    }

    #[test]
    fn activation_relu_mlir() {
        let pattern = KernelPattern::Activation {
            op: ActivationOp::Relu,
            input: make_tensor("x", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            dim_name: "N".into(),
        };
        let mlir = build_mlir(&pattern, "relu").unwrap();
        assert!(mlir.contains("stablehlo.maximum"));
    }

    #[test]
    fn activation_tanh_mlir() {
        let pattern = KernelPattern::Activation {
            op: ActivationOp::Tanh,
            input: make_tensor("x", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            dim_name: "N".into(),
        };
        let mlir = build_mlir(&pattern, "tanh_test").unwrap();
        assert!(mlir.contains("stablehlo.tanh"));
    }

    #[test]
    fn pool_max_mlir() {
        let pattern = KernelPattern::Pool {
            kind: PoolKind::Max,
            input: make_tensor("x", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            shape: PoolShape {
                kernel_h: 2,
                kernel_w: 2,
                stride_h: 2,
                stride_w: 2,
            },
        };
        let mlir = build_mlir(&pattern, "maxpool").unwrap();
        assert!(mlir.contains("stablehlo.reduce_window"));
    }

    #[test]
    fn reduce_sum_mlir() {
        let pattern = KernelPattern::Reduce {
            op: ReduceOp::Sum,
            input: make_tensor("x", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            axis: 1,
        };
        let mlir = build_mlir(&pattern, "reduce_sum").unwrap();
        assert!(mlir.contains("stablehlo.reduce"));
    }

    #[test]
    fn transpose_mlir() {
        let pattern = KernelPattern::Transpose {
            input: make_tensor("x", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            perm: vec![1, 0],
        };
        let mlir = build_mlir(&pattern, "transpose").unwrap();
        assert!(mlir.contains("stablehlo.transpose"));
    }

    #[test]
    fn normalization_mlir() {
        let pattern = KernelPattern::Normalization {
            input: make_tensor("x", TensorRole::Input),
            scale: make_tensor("gamma", TensorRole::Input),
            bias: make_tensor("beta", TensorRole::Input),
            output: make_tensor("y", TensorRole::Output),
            epsilon: 1e-5,
            norm_type: NormType::Batch,
        };
        let mlir = build_mlir(&pattern, "batchnorm").unwrap();
        assert!(mlir.contains("stablehlo.batch_norm_inference"));
    }
}
